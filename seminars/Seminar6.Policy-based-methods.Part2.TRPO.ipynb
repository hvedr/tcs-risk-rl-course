{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "Seminar_TRPO_risks.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4A-AYCUmcpC",
        "colab_type": "code",
        "outputId": "bf5176a0-fabf-4886-95ac-9ebd0f3b3934",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# этот ноутбук в google colab \n",
        "# https://colab.research.google.com/drive/1_oV3Pz-nZaYzoo4BZ-HOA3J4eKqk74x5\n",
        "\n",
        "# in google colab uncomment this\n",
        "\n",
        "import os\n",
        "\n",
        "os.system('apt-get install -y xvfb')\n",
        "os.system('wget https://raw.githubusercontent.com/yandexdataschool/Practical_DL/fall18/xvfb -O ../xvfb')\n",
        "os.system('apt-get install -y python-opengl ffmpeg')\n",
        "os.system('pip install pyglet==1.2.4')\n",
        "\n",
        "# launch XVFB if you run on a server\n",
        "import os\n",
        "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
        "    !bash ../xvfb start\n",
        "    %env DISPLAY = : 1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting virtual X frame buffer: Xvfb.\n",
            "env: DISPLAY=: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Syb30gDlmcpH",
        "colab_type": "text"
      },
      "source": [
        "### Let's make a TRPO!\n",
        "\n",
        "In this notebook we will write the code of the one Trust Region Policy Optimization.\n",
        "As usually, it contains a few different parts which we are going to reproduce.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFtIpEUimcpI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shLHiQ1XmcpM",
        "colab_type": "code",
        "outputId": "b9c0a06d-d0c7-4328-8962-03609999b123",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import gym\n",
        "\n",
        "env = gym.make(\"Acrobot-v1\")\n",
        "env.reset()\n",
        "observation_shape = env.observation_space.shape\n",
        "n_actions = env.action_space.n\n",
        "print(\"Observation Space\", env.observation_space)\n",
        "print(\"Action Space\", env.action_space)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Observation Space Box(6,)\n",
            "Action Space Discrete(3)\n",
            "Observation Space Box(6,)\n",
            "Action Space Discrete(3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqRwoIJ3mcpR",
        "colab_type": "code",
        "outputId": "adfd7640-cd3d-4731-d514-d27dc022b694",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.imshow(env.render('rgb_array'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f28aabfd160>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQsAAAD8CAYAAABgtYFHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFpK6GBaSuvw/oFD3477h6OUAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6skHqGqAmcpU",
        "colab_type": "text"
      },
      "source": [
        "### Step 1: Defining a network\n",
        "\n",
        "With all it's complexity, at it's core TRPO is yet another policy gradient method. \n",
        "\n",
        "This essentially means we're actually training a stochastic policy $ \\pi_\\theta(a|s) $. \n",
        "\n",
        "And yes, it's gonna be a neural network. So let's start by defining one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAwm4FUKmcpV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TRPOAgent(nn.Module):\n",
        "    def __init__(self, state_shape, n_actions, hidden_size=128):\n",
        "        '''\n",
        "        Here you should define your model\n",
        "        You should have LOG-PROBABILITIES as output because you will need it to compute loss\n",
        "        We recommend that you start simple: \n",
        "        use 1-2 hidden layers with 100-500 units and relu for the first try\n",
        "        '''\n",
        "        super(TRPOAgent,self).__init__()\n",
        "\n",
        "        \n",
        "        self.model = nn.Sequential(nn.Linear(state_shape[0], hidden_size),\n",
        "                     nn.ReLU(),\n",
        "                     nn.Linear(hidden_size, hidden_size),\n",
        "                     nn.ReLU(),\n",
        "                      nn.Linear(hidden_size, n_actions),\n",
        "                     nn.LogSoftmax())\n",
        "\n",
        "    def forward(self, states):\n",
        "        \"\"\"\n",
        "        takes agent's observation (Variable), returns log-probabilities (Variable)\n",
        "        :param state_t: a batch of states, shape = [batch_size, state_shape]\n",
        "        \"\"\"\n",
        "\n",
        "        # Use your network to compute log_probs for given state\n",
        "        log_probs = self.model(states)\n",
        "        return log_probs\n",
        "\n",
        "    def get_log_probs(self, states):\n",
        "        '''\n",
        "        Log-probs for training\n",
        "        '''\n",
        "\n",
        "        return self.forward(states)\n",
        "\n",
        "    def get_probs(self, states):\n",
        "        '''\n",
        "        Probs for interaction\n",
        "        '''\n",
        "\n",
        "        return torch.exp(self.forward(states))\n",
        "\n",
        "    def act(self, obs, sample=True):\n",
        "        '''\n",
        "        Samples action from policy distribution (sample = True) or takes most likely action (sample = False)\n",
        "        :param: obs - single observation vector\n",
        "        :param sample: if True, samples from \\pi, otherwise takes most likely action\n",
        "        :returns: action (single integer) and probabilities for all actions\n",
        "        '''\n",
        "\n",
        "        probs = self.get_probs(torch.FloatTensor([obs])).data.numpy()\n",
        "\n",
        "        if sample:\n",
        "            action = int(np.random.choice(n_actions, p=probs[0]))\n",
        "        else:\n",
        "            action = int(np.argmax(probs))\n",
        "\n",
        "        return action, probs[0]\n",
        "\n",
        "\n",
        "agent = TRPOAgent(observation_shape, n_actions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fshUKSgImcpX",
        "colab_type": "code",
        "outputId": "4bf06578-4149-4707-eb43-29d6fb98c201",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "# Check if log-probabilities satisfies all the requirements\n",
        "log_probs = agent.get_log_probs(torch.FloatTensor([env.reset()]))\n",
        "assert log_probs.requires_grad, \"qvalues must be a torch tensor with grad\"\n",
        "assert len(\n",
        "    log_probs.shape) == 2 and log_probs.shape[0] == 1 and log_probs.shape[1] == n_actions\n",
        "sums = torch.sum(torch.exp(log_probs), dim=1)\n",
        "assert (0.999 < sums).all() and (1.001 > sums).all()\n",
        "\n",
        "# Demo use\n",
        "print(\"sampled:\", [agent.act(env.reset()) for _ in range(5)])\n",
        "print(\"greedy:\", [agent.act(env.reset(), sample=False) for _ in range(5)])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sampled: [(1, array([0.27565548, 0.3772354 , 0.34710908], dtype=float32)), (0, array([0.2747131 , 0.37713453, 0.3481524 ], dtype=float32)), (2, array([0.2750938 , 0.37725896, 0.34764716], dtype=float32)), (0, array([0.27430654, 0.3781341 , 0.34755936], dtype=float32)), (2, array([0.27562425, 0.37723002, 0.34714574], dtype=float32))]\n",
            "greedy: [(1, array([0.27522305, 0.3770664 , 0.3477105 ], dtype=float32)), (1, array([0.27549174, 0.3778428 , 0.34666538], dtype=float32)), (1, array([0.27634597, 0.3777832 , 0.34587088], dtype=float32)), (1, array([0.27602547, 0.3780179 , 0.34595662], dtype=float32)), (1, array([0.27447042, 0.37826294, 0.34726664], dtype=float32))]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZGFEpxamcpZ",
        "colab_type": "text"
      },
      "source": [
        "#### Flat parameters operations\n",
        "\n",
        "We are going to use it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qj4_6HTrmcpc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_flat_params_from(model):\n",
        "    params = []\n",
        "    for param in model.parameters():\n",
        "        params.append(param.data.view(-1))\n",
        "\n",
        "    flat_params = torch.cat(params)\n",
        "    return flat_params\n",
        "\n",
        "\n",
        "def set_flat_params_to(model, flat_params):\n",
        "    prev_ind = 0\n",
        "    for param in model.parameters():\n",
        "        flat_size = int(np.prod(list(param.size())))\n",
        "        param.data.copy_(\n",
        "            flat_params[prev_ind:prev_ind + flat_size].view(param.size()))\n",
        "        prev_ind += flat_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLv0wY0Amcpe",
        "colab_type": "text"
      },
      "source": [
        "Compute cumulative reward just like you did in vanilla REINFORCE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0899MulQmcpe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def get_cumulative_returns(r, gamma=1):\n",
        "    \"\"\"\n",
        "    Computes cumulative discounted rewards given immediate rewards\n",
        "    G_i = r_i + gamma*r_{i+1} + gamma^2*r_{i+2} + ...\n",
        "    Also known as R(s,a).\n",
        "    \"\"\"\n",
        "    r = np.array(r)\n",
        "    assert r.ndim >= 1\n",
        "    return np.array([np.dot( gamma**np.arange(0,len(r)-i+0), r[i:] ) for i in range(len(r))])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4ex5Vutmcpg",
        "colab_type": "code",
        "outputId": "d60a9d72-3eee-4db3-9b26-5899a630207d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# simple demo on rewards [0,0,1,0,0,1]\n",
        "get_cumulative_returns([0, 0, 1, 0, 0, 1], gamma=0.9)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.40049, 1.5561 , 1.729  , 0.81   , 0.9    , 1.     ])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_owrduamcpi",
        "colab_type": "text"
      },
      "source": [
        "**Rollout**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4eB6LPDmcpj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rollout(env, agent, max_pathlength=2500, n_timesteps=50000):\n",
        "    \"\"\"\n",
        "    Generate rollouts for training.\n",
        "    :param: env - environment in which we will make actions to generate rollouts.\n",
        "    :param: act - the function that can return policy and action given observation.\n",
        "    :param: max_pathlength - maximum size of one path that we generate.\n",
        "    :param: n_timesteps - total sum of sizes of all pathes we generate.\n",
        "    \"\"\"\n",
        "    paths = []\n",
        "\n",
        "    total_timesteps = 0\n",
        "    while total_timesteps < n_timesteps:\n",
        "        observations, actions, rewards, action_probs = [], [], [], []\n",
        "        observation = env.reset()\n",
        "        for _ in range(max_pathlength):\n",
        "            action, policy = agent.act(observation)\n",
        "            observations.append(observation)\n",
        "            actions.append(action)\n",
        "            action_probs.append(policy)\n",
        "            observation, reward, done, _ = env.step(action)\n",
        "            rewards.append(reward)\n",
        "            total_timesteps += 1\n",
        "            if done or total_timesteps == n_timesteps:\n",
        "                path = {\"observations\": np.array(observations),\n",
        "                        \"policy\": np.array(action_probs),\n",
        "                        \"actions\": np.array(actions),\n",
        "                        \"rewards\": np.array(rewards),\n",
        "                        \"cumulative_returns\": get_cumulative_returns(rewards),\n",
        "                        }\n",
        "                paths.append(path)\n",
        "                break\n",
        "    return paths"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8F0dn-ymcpk",
        "colab_type": "code",
        "outputId": "7f71ffdb-2f6f-4838-f2e3-8159d83c6fd0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "paths = rollout(env, agent, max_pathlength=5, n_timesteps=100)\n",
        "print(paths[-1])\n",
        "assert (paths[0]['policy'].shape == (5, n_actions))\n",
        "assert (paths[0]['cumulative_returns'].shape == (5,))\n",
        "assert (paths[0]['rewards'].shape == (5,))\n",
        "assert (paths[0]['observations'].shape == (5,)+observation_shape)\n",
        "assert (paths[0]['actions'].shape == (5,))\n",
        "print('It\\'s ok')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'observations': array([[ 0.99786821, -0.06526131,  0.9998829 , -0.01530345,  0.04826368,\n",
            "        -0.03406411],\n",
            "       [ 0.998096  , -0.06167963,  0.99996564,  0.00829017, -0.01224285,\n",
            "         0.26621897],\n",
            "       [ 0.99759776, -0.06927277,  0.99618967,  0.0872132 , -0.05961429,\n",
            "         0.50889464],\n",
            "       [ 0.99838579, -0.0567963 ,  0.99082509,  0.13515042,  0.18304017,\n",
            "        -0.0319071 ],\n",
            "       [ 0.99992319, -0.0123939 ,  0.99380295,  0.11115618,  0.25143911,\n",
            "        -0.19876378]]), 'policy': array([[0.27487838, 0.37693143, 0.34819034],\n",
            "       [0.2753873 , 0.37685126, 0.34776148],\n",
            "       [0.27608177, 0.37691057, 0.3470076 ],\n",
            "       [0.273094  , 0.37989166, 0.34701437],\n",
            "       [0.2719149 , 0.38020763, 0.34787747]], dtype=float32), 'actions': array([2, 2, 0, 1, 1]), 'rewards': array([-1., -1., -1., -1., -1.]), 'cumulative_returns': array([-5., -4., -3., -2., -1.])}\n",
            "It's ok\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fj9Ecb6mcpm",
        "colab_type": "text"
      },
      "source": [
        "### Step 3: Auxiliary functions\n",
        "\n",
        "Now let's define the loss functions and something else for actual TRPO training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIIKEjHjmcpn",
        "colab_type": "text"
      },
      "source": [
        "The surrogate reward should be\n",
        "$$J_{surr}= {1 \\over N} \\sum\\limits_{i=0}^N \\frac{\\pi_{\\theta}(s_i, a_i)}{\\pi_{\\theta_{old}}(s_i, a_i)}A_{\\theta_{old}(s_i, a_i)}$$\n",
        "\n",
        "For simplicity, let's use cumulative returns instead of advantage for now:\n",
        "$$J'_{surr}= {1 \\over N} \\sum\\limits_{i=0}^N \\frac{\\pi_{\\theta}(s_i, a_i)}{\\pi_{\\theta_{old}}(s_i, a_i)}G_{\\theta_{old}(s_i, a_i)}$$\n",
        "\n",
        "Or alternatively, minimize the surrogate loss:\n",
        "$$ L_{surr} = - J'_{surr} $$  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zn4kdi9Dmcpn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_loss(agent, observations, actions, cumulative_returns, old_probs):\n",
        "    \"\"\"\n",
        "    Computes TRPO objective\n",
        "    :param: observations - batch of observations\n",
        "    :param: actions - batch of actions\n",
        "    :param: cumulative_returns - batch of cumulative returns\n",
        "    :param: old_probs - batch of probabilities computed by old network\n",
        "    :returns: scalar value of the objective function\n",
        "    \"\"\"\n",
        "    batch_size = observations.shape[0]\n",
        "#     log_probs_all = agent.get_log_probs(observations)\n",
        "#     probs_all = torch.exp(log_probs_all) # заменить на get_probs\n",
        "    probs_all = agent.get_probs(observations)\n",
        "\n",
        "    probs_for_actions = probs_all[torch.arange(\n",
        "        0, batch_size, out=torch.LongTensor()), actions]\n",
        "    old_probs_for_actions = old_probs[torch.arange(\n",
        "        0, batch_size, out=torch.LongTensor()), actions]\n",
        "\n",
        "    # Compute surrogate loss, aka importance-sampled policy gradient\n",
        "#     Loss = \n",
        "    assert Loss.shape == torch.Size([])\n",
        "    return Loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1Vl4XP4mcpr",
        "colab_type": "text"
      },
      "source": [
        "We can ascend these gradients as long as our $\\pi_\\theta(a|s)$ satisfies the constraint\n",
        "$$E_{s,\\pi_{\\Theta_{t}}}\\Big[KL(\\pi(\\Theta_{t}, s) \\:||\\:\\pi(\\Theta_{t+1}, s))\\Big]< \\alpha$$\n",
        "\n",
        "\n",
        "where\n",
        "\n",
        "$$KL(p||q) = E _p log({p \\over q})$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6ad3fNImcps",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_kl(agent, observations, actions, cumulative_returns, old_probs):\n",
        "    \"\"\"\n",
        "    Computes KL-divergence between network policy and old policy\n",
        "    :param: observations - batch of observations\n",
        "    :param: actions - batch of actions\n",
        "    :param: cumulative_returns - batch of cumulative returns (we don't need it actually)\n",
        "    :param: old_probs - batch of probabilities computed by old network\n",
        "    :returns: scalar value of the KL-divergence\n",
        "    \"\"\"\n",
        "    batch_size = observations.shape[0]\n",
        "    log_probs_all = agent.get_log_probs(observations)\n",
        "    probs_all = torch.exp(log_probs_all)\n",
        "\n",
        "    # Compute Kullback-Leibler divergence (see formula above)\n",
        "    # Note: you need to sum KL and entropy over all actions, not just the ones agent took\n",
        "    old_log_probs = torch.log(old_probs+1e-10)\n",
        "    \n",
        "#     print((old_log_probs-log_probs_all),probs_all)\n",
        "\n",
        "#     kl = \n",
        "#     print(torch.sum((old_log_probs-log_probs_all)*probs_all))\n",
        "    assert kl.shape == torch.Size([])\n",
        "    assert (kl > -0.0001).all() and (kl < 10000).all()\n",
        "    return kl"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SonP05E4mcpu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_entropy(agent, observations):\n",
        "    \"\"\"\n",
        "    Computes entropy of the network policy \n",
        "    :param: observations - batch of observations\n",
        "    :returns: scalar value of the entropy\n",
        "    \"\"\"\n",
        "\n",
        "    observations = torch.FloatTensor(observations)\n",
        "\n",
        "    batch_size = observations.shape[0]\n",
        "    log_probs_all = agent.get_log_probs(observations)\n",
        "    probs_all = torch.exp(log_probs_all)\n",
        "\n",
        "    entropy = torch.sum(-probs_all * log_probs_all) / batch_size\n",
        "\n",
        "    assert entropy.shape == torch.Size([])\n",
        "    return entropy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cYK1boZmcpx",
        "colab_type": "text"
      },
      "source": [
        "**Linear search**\n",
        "\n",
        "TRPO in its core involves ascending surrogate policy gradient constrained by KL divergence. \n",
        "\n",
        "In order to enforce this constraint, we're gonna use linesearch. You can find out more about it [here](https://en.wikipedia.org/wiki/Linear_search)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlPCOISymcpx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def linesearch(f, x, fullstep, max_kl):\n",
        "    \"\"\"\n",
        "    Linesearch finds the best parameters of neural networks in the direction of fullstep contrainted by KL divergence.\n",
        "    :param: f - function that returns loss, kl and arbitrary third component.\n",
        "    :param: x - old parameters of neural network.\n",
        "    :param: fullstep - direction in which we make search.\n",
        "    :param: max_kl - constraint of KL divergence.\n",
        "    :returns:\n",
        "    \"\"\"\n",
        "    max_backtracks = 10\n",
        "    loss, _, = f(x)\n",
        "    for stepfrac in .5**np.arange(max_backtracks):\n",
        "        xnew = x + stepfrac * fullstep\n",
        "        new_loss, kl = f(xnew)\n",
        "        actual_improve = new_loss - loss\n",
        "        if kl.data.numpy() <= max_kl and actual_improve.data.numpy() < 0:\n",
        "            x = xnew\n",
        "            loss = new_loss\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bh6_C1vPmcpz",
        "colab_type": "text"
      },
      "source": [
        "**Conjugate gradients**\n",
        "\n",
        "Since TRPO includes contrainted optimization, we will need to solve Ax=b using conjugate gradients.\n",
        "\n",
        "In general, CG is an algorithm that solves Ax=b where A is positive-defined. A is Hessian matrix so A is positive-defined. You can find out more about them [here](https://en.wikipedia.org/wiki/Conjugate_gradient_method)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRh1Bldumcp1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from numpy.linalg import inv\n",
        "\n",
        "\n",
        "def conjugate_gradient(f_Ax, b, cg_iters=10, residual_tol=1e-10):\n",
        "    \"\"\"\n",
        "    This method solves system of equation Ax=b using iterative method called conjugate gradients\n",
        "    :f_Ax: function that returns Ax\n",
        "    :b: targets for Ax\n",
        "    :cg_iters: how many iterations this method should do\n",
        "    :residual_tol: epsilon for stability\n",
        "    \"\"\"\n",
        "    p = b.clone()\n",
        "    r = b.clone()\n",
        "    x = torch.zeros(b.size())\n",
        "    rdotr = torch.sum(r*r)\n",
        "    for i in range(cg_iters):\n",
        "        z = f_Ax(p)\n",
        "        v = rdotr / (torch.sum(p*z) + 1e-8)\n",
        "        x += v * p\n",
        "        r -= v * z\n",
        "        newrdotr = torch.sum(r*r)\n",
        "        mu = newrdotr / (rdotr + 1e-8)\n",
        "        p = r + mu * p\n",
        "        rdotr = newrdotr\n",
        "        if rdotr < residual_tol:\n",
        "            break\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWfOLAgJmcp4",
        "colab_type": "code",
        "outputId": "e18eabeb-e7a6-4c7d-c73d-fa7b19fd2e1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# This code validates conjugate gradients\n",
        "A = np.random.rand(8, 8)\n",
        "A = np.matmul(np.transpose(A), A)\n",
        "\n",
        "\n",
        "def f_Ax(x):\n",
        "    return torch.matmul(torch.FloatTensor(A), x.view((-1, 1))).view(-1)\n",
        "\n",
        "\n",
        "b = np.random.rand(8)\n",
        "\n",
        "w = np.matmul(np.matmul(inv(np.matmul(np.transpose(A), A)),\n",
        "                        np.transpose(A)), b.reshape((-1, 1))).reshape(-1)\n",
        "print(w)\n",
        "print(conjugate_gradient(f_Ax, torch.FloatTensor(b)).numpy())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[  5.64388351  -2.50578312  -3.0362097   -2.79947014   5.32822463\n",
            "   0.4432495  -11.50473204   4.86481194]\n",
            "[  5.641274    -2.5021398   -3.039077    -2.7970712    5.328253\n",
            "   0.43931255 -11.505307     4.865808  ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lpymT2uh0WU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n= 3000\n",
        "A = np.random.rand(n, n)\n",
        "A = np.matmul(np.transpose(A), A)\n",
        "\n",
        "b = np.random.rand(n)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwdNN5zBhliL",
        "colab_type": "code",
        "outputId": "16a9b5b4-d400-47bc-d9d9-bd759e854a59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "w = np.matmul(np.matmul(inv(np.matmul(np.transpose(A), A)),\n",
        "                        np.transpose(A)), b.reshape((-1, 1))).reshape(-1)\n",
        "# print(w)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 9.5 s, sys: 2.08 s, total: 11.6 s\n",
            "Wall time: 8.9 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YaN9hAWRhlmK",
        "colab_type": "code",
        "outputId": "f258e30d-8779-4628-98f9-6a3e93b847be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "w = conjugate_gradient(f_Ax, torch.FloatTensor(b)).numpy()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 223 ms, sys: 69.1 ms, total: 292 ms\n",
            "Wall time: 256 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiVyWFcRmcp6",
        "colab_type": "text"
      },
      "source": [
        "### Step 4: training\n",
        "In this section we construct the whole update step function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHIdmNfMmcp7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def update_step(agent, observations, actions, cumulative_returns, old_probs, max_kl):\n",
        "    \"\"\"\n",
        "    This function does the TRPO update step\n",
        "    :param: observations - batch of observations\n",
        "    :param: actions - batch of actions\n",
        "    :param: cumulative_returns - batch of cumulative returns\n",
        "    :param: old_probs - batch of probabilities computed by old network\n",
        "    :param: max_kl - controls how big KL divergence may be between old and new policy every step.\n",
        "    :returns: KL between new and old policies and the value of the loss function.\n",
        "    \"\"\"\n",
        "\n",
        "    # Here we prepare the information\n",
        "    observations = torch.FloatTensor(observations)\n",
        "    actions = torch.LongTensor(actions)\n",
        "    cumulative_returns = torch.FloatTensor(cumulative_returns)\n",
        "    old_probs = torch.FloatTensor(old_probs)\n",
        "\n",
        "    # Here we compute gradient of the loss function\n",
        "    loss = get_loss(agent, observations, actions,\n",
        "                    cumulative_returns, old_probs)\n",
        "    grads = torch.autograd.grad(loss, agent.parameters())\n",
        "    loss_grad = torch.cat([grad.view(-1) for grad in grads]).data\n",
        "\n",
        "    def Fvp(v):\n",
        "        # Here we compute Fx to do solve Fx = g using conjugate gradients\n",
        "        # We actually do here a couple of tricks to compute it efficiently\n",
        "\n",
        "        kl = get_kl(agent, observations, actions,\n",
        "                    cumulative_returns, old_probs)\n",
        "\n",
        "        grads = torch.autograd.grad(kl, agent.parameters(), create_graph=True)\n",
        "        flat_grad_kl = torch.cat([grad.view(-1) for grad in grads])\n",
        "\n",
        "        kl_v = (flat_grad_kl * v).sum()\n",
        "        grads = torch.autograd.grad(kl_v, agent.parameters())\n",
        "        flat_grad_grad_kl = torch.cat(\n",
        "            [grad.contiguous().view(-1) for grad in grads]).data\n",
        "\n",
        "        return flat_grad_grad_kl + v * 0.1\n",
        "\n",
        "    # Here we solve Fx = g system using conjugate gradients\n",
        "    stepdir = conjugate_gradient(Fvp, -loss_grad, 10)\n",
        "\n",
        "    # Here we compute the initial vector to do linear search\n",
        "    shs = 0.5 * (stepdir * Fvp(stepdir)).sum(0, keepdim=True)\n",
        "\n",
        "    lm = torch.sqrt(shs / max_kl)\n",
        "    fullstep = stepdir / lm[0]\n",
        "\n",
        "#     neggdotstepdir = (-loss_grad * stepdir).sum(0, keepdim=True)\n",
        "\n",
        "    # Here we get the start point\n",
        "    prev_params = get_flat_params_from(agent)\n",
        "\n",
        "    def get_loss_kl(params):\n",
        "        # Helper for linear search\n",
        "        set_flat_params_to(agent, params)\n",
        "        return [get_loss(agent, observations, actions, cumulative_returns, old_probs),\n",
        "                get_kl(agent, observations, actions, cumulative_returns, old_probs)]\n",
        "\n",
        "    # Here we find our new parameters\n",
        "    new_params = linesearch(get_loss_kl, prev_params, fullstep, max_kl)\n",
        "\n",
        "    # And we set it to our network\n",
        "    set_flat_params_to(agent, new_params)\n",
        "\n",
        "    return get_loss_kl(new_params)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Ucghdhwmcp9",
        "colab_type": "text"
      },
      "source": [
        "##### Step 5: Main TRPO loop\n",
        "\n",
        "Here we will train our network!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03BU7fmEmcp9",
        "colab_type": "code",
        "outputId": "e41c2324-049d-4f18-a284-ffc7038e1524",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "import time\n",
        "from itertools import count\n",
        "from collections import OrderedDict\n",
        "\n",
        "# this is hyperparameter of TRPO. It controls how big KL divergence may be between old and new policy every step.\n",
        "max_kl = 0.01\n",
        "numeptotal = 0  # this is number of episodes that we played.\n",
        "\n",
        "start_time = time.time()\n",
        "agent = TRPOAgent(observation_shape, n_actions)\n",
        "\n",
        "for i in count(1):\n",
        "\n",
        "    print(\"\\n********** Iteration %i ************\" % i)\n",
        "\n",
        "    # Generating paths.\n",
        "    print(\"Rollout\")\n",
        "    paths = rollout(env, agent)\n",
        "    print(\"Made rollout\")\n",
        "\n",
        "    # Updating policy.\n",
        "    observations = np.concatenate([path[\"observations\"] for path in paths])\n",
        "    actions = np.concatenate([path[\"actions\"] for path in paths])\n",
        "    returns = np.concatenate([path[\"cumulative_returns\"] for path in paths])\n",
        "    old_probs = np.concatenate([path[\"policy\"] for path in paths])\n",
        "\n",
        "    loss, kl = update_step(agent, observations, actions,\n",
        "                           returns, old_probs, max_kl)\n",
        "\n",
        "    # Report current progress\n",
        "    episode_rewards = np.array([path[\"rewards\"].sum() for path in paths])\n",
        "\n",
        "    stats = OrderedDict()\n",
        "    numeptotal += len(episode_rewards)\n",
        "    stats[\"Total number of episodes\"] = numeptotal\n",
        "    stats[\"Average sum of rewards per episode\"] = episode_rewards.mean()\n",
        "    stats[\"Std of rewards per episode\"] = episode_rewards.std()\n",
        "    stats[\"Time elapsed\"] = \"%.2f mins\" % ((time.time() - start_time)/60.)\n",
        "    stats[\"KL between old and new distribution\"] = kl.data.numpy()\n",
        "    stats[\"Entropy\"] = get_entropy(agent, observations).data.numpy()\n",
        "    stats[\"Surrogate loss\"] = loss.data.numpy()\n",
        "    for k, v in stats.items():\n",
        "        print(k + \": \" + \" \" * (40 - len(k)) + str(v))\n",
        "    i += 1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "********** Iteration 1 ************\n",
            "Rollout\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Made rollout\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-6a83151e3c99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     loss, kl = update_step(agent, observations, actions,\n\u001b[0;32m---> 28\u001b[0;31m                            returns, old_probs, max_kl)\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m# Report current progress\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-95d988ce80f3>\u001b[0m in \u001b[0;36mupdate_step\u001b[0;34m(agent, observations, actions, cumulative_returns, old_probs, max_kl)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# Here we compute gradient of the loss function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     loss = get_loss(agent, observations, actions,\n\u001b[0;32m---> 20\u001b[0;31m                     cumulative_returns, old_probs)\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mloss_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mgrad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-f6d910e7c4bf>\u001b[0m in \u001b[0;36mget_loss\u001b[0;34m(agent, observations, actions, cumulative_returns, old_probs)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# Compute surrogate loss, aka importance-sampled policy gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m#     Loss =\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mLoss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mLoss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Loss' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "kEdWKAMJmcp_",
        "colab_type": "text"
      },
      "source": [
        "# Homework\n",
        "\n",
        "- Переделать алгоритм TRPO на PPO: убрать сложную оптимизацию, добавить критика, изменить лосс\n",
        "- Добавить поддержку непрерывных действий\n",
        "- Обучить PPO на py-bullet (бесплатный аналог MuJoCo)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJDOcmIBluOB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/benelot/pybullet-gym lib/pybullet-gym\n",
        "# !pip install -e lib/pybullet-gym\n",
        "!pip install lib/pybullet-gym\n",
        "! cp -r lib/pybullet-gym/pybulletgym/envs/* /usr/local/lib/python3.6/dist-packages/pybulletgym/envs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOG0-ymxlwVK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym \n",
        "import pybulletgym\n",
        "\n",
        "env = gym.make(\"HalfCheetahMuJoCoEnv-v0\")\n",
        "print(\"observation space: \", env.observation_space,\n",
        "      \"\\nobservations:\", env.reset())\n",
        "print(\"action space: \", env.action_space, \n",
        "      \"\\naction_sample: \", env.action_space.sample())"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
